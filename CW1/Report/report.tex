	\documentclass[conference]{IEEEtran}

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subfigure}
\usepackage{float}
\usepackage{indentfirst}
\usepackage[compact]{titlesec}
\setlength{\parindent}{11pt}
\newcommand{\forceindent}{\leavevmode{\parindent=1em\indent}}

\hyphenation{op-tical net-works semi-conduc-tor}

\graphicspath{{images/}}

\usepackage{hyperref}

\hypersetup
{
    colorlinks=true,
    linkcolor=black,   
    urlcolor=blue,
    citecolor=black,
}


\begin{document}
\title{AINT308 - OpenCV Assignment 1 2022}

\author{\IEEEauthorblockN{Student No. 10618407}
\IEEEauthorblockA{School of Engineering,\\Computing and Mathematics
\\University of Plymouth\\
Plymouth, Devon\\}}

\maketitle

\begin{abstract}

Machine learning is not a new technology, it is a field that is becoming more prevalent within modern engineering practises. It is being used more in the rapidly evolving fields of autonomy and automation. This report outlines some of the functionality of a popular C based computer visions library \href{https://opencv.org}{OpenCV}. Task 1 is to evaluate the colors of pixels in a picture to determine the colour of a given object in the frame (car). Task 2 
was to track on object across frames of a video to track its motion (swinging pendulum). Task 3 was to identify and cross correlate components on a circuit to check for any missing components.

\end{abstract}

\subsection*{Keywords:} 
Computer Vision, OpenCV, Object Detection, RGB\\

\section{Task 1: Colour Sorter}	
\subsection{Introduction}
The first task was to evaluate the colour of a given object in a given frame color within the RGB colour space. 
The task was to identify the colour of a given car. An example of the type of image can be seen in Fig. \ref{fig:example_car}.

\begin{figure}[H]
\centerline{\includegraphics[width=.4\textwidth]{example_car}}
\caption{Example Car Image}
\label{fig:example_car}
\end{figure}

\subsection{Solution}
Fig \ref{fig:CW1_Task_1_code_colour_checker} below shows the solution to the first task, the Colour Sorter.

\begin{figure}[H]
\centerline{\includegraphics[width=.5\textwidth]{CW1_Task_1_code_colour_checker}}
\caption{Task 1 Code - Colour Checking}
\label{fig:CW1_Task_1_code_colour_checker}
\end{figure}

The code iterates through two loops, one to look at all the rows and the second to go through all the columns in a given row. 
The value of each pixel is the evaluated, if the R(red), G(green), or B(blue) value if more than $1.5$ times larger than the other R, G, B values, then the pixel is deemed to be that colour. 
The colour of this pixel is added to a running total for each of the colours. 

After the whole image is checked the values are checked and the highest value is deemed to be the colour of the car. This can be seen in Fig. \ref{fig:CW1_Task_1_code_colour_checker_output}.

\begin{figure}[H]
\centerline{\includegraphics[width=.5\textwidth]{CW1_Task_1_code_colour_checker_output}}
\caption{Task 1 Code - Output}
\label{fig:CW1_Task_1_code_colour_checker_output}
\end{figure}
To test the algorithm further, a large dataset of pictures of cars was used. 40 more images of cars was added to the dataset, alongside the original 30 that was provided. The new dataset was initially used to train and test machine learning models. \cite{ref:KrauseStarkDengFei-Fei_3DRR2013} These were added to test how to functionality of the task held up without highly cropped and optimised inputs.

Overall this method worked for the dataset that was provided, but not well for the added dataset. 
This dataset was selected so that the car made up a majority of the frame and the background were, for the most part plain and didn't sway the results.


\subsection{Further Improvements}
Although this task worked for the given dataset, there are some ways to improve the accuracy. RGB models are typically not used for this type of task, with the Hue, Saturation, and Value (HSV) model being preferred. Initially the RGB model was built and optimised to be used on display screens, whereas the HSV model was developed to mimic how a human interprets colours. Since HSV models are able to separate colour vales and lightness, operations can be more easily performed on the hue itself. Because we are more interested in the colour of the cars and not the lightness, a HSV colour model would be a objectively better solution. \cite{ref:HSV_vs_RGB}

Another improvement is to make the algorithm detect the outline of the car. This would eliminate the issues caused by having a background that could be mistaken for the body of the car. An example of this can be seen in Fig \ref{fig:red_car_green_background}. Open CV is capable of detecting cars in images and videos, albeit trivially. Although trivial, it could be used to drawe a box around the car thus eliminating a lot of the background surrounding the car. \cite{ref:Vehicle_detection_in_python} 

\begin{figure}[H]
\centerline{\includegraphics[width=.5\textwidth]{red_car_green_background}}
\caption{Example Car with undesirable background}
\label{fig:red_car_green_background}
\end{figure}

The red car with a green background may confuse the simple sorting algorithm and lead to a false result (the algorithm may think the car is green). This is the issue with using such a basic classification algorithm.

The final improvement that could be partially solved by using the HSV colour model, but that would also require a far more complex solving algorithm \cite{ref:colour_sorting}, is that cars are not just red, green, or blue. They come in many different colours (including white, black, and silver) and also in many different shades and brightness of colour. By isolating just the RGB values and evaluating which is highest, a lot of granularity is lost. Using this method it is impossible to detect the shade of the car and even the type of red, green, or blue the car may be, if it is one of these colours at all. Using the blend of colours and evaluating them against known RGB values, further colours of car could have been identified with a significantly higher range of colours being available (Up to 16M different combinations for an 8-bit RGB value). This method however would require some form of 'lookup table' that would be used to compare the RGB pixel value to. This adds complexity to the task, but allowed for a better overall result.

\subsection{Conclusion}

Overall this task was successful for the limited dataset that was provided. This dataset only included Red, Green, and Blue cars. These cars were placed in centre of the frame with neutral lighting. Due to the limited natured of both the colour space sued and the approach, anything more substantial than minor performance increased would require a overhaul of the methodology used. Changing from the \textit{RGB} to the \textit{HSV} colour space would increase the accuracy of the colour detection and would allow the solution to be expanded to a fuller range of car colours. Finally, outlining the car and omitting the background would reduce the likelihood of the background of the image influencing the detected colour of the car.

\section{Task 2: Colour Tracker}

\subsection{Introduction}

The task was a colour tracker, that was set to track a specific colour in the frame. This colour was attached to the end of a pendulum. The results were recorded and the relative angle of the pendulum was recorded. This angle was recorded in both radians and degrees. 

\subsection{Solution}
Figure \ref{fig:CW1_Task_2_code_locating_rectange} is the code used to locate the colour in the image. 

\begin{figure}
\centerline{\includegraphics[width=.5\textwidth]{CW1_Task_2_code_locating_rectange}}
\caption{Colour Location Code}
\label{fig:CW1_Task_2_code_locating_rectange}
\end{figure}

The colour in the frame was first converted from the RGB colour space to the HSV colour space, a still of this can be seen in Appendix \ref{appendix:pendulum_rgb_hsv_space}. The justification for this has been outlined above, it makes colours easier to identify in images \cite{ref:HSV_vs_RGB}. After identifying all of the correctly coloured pixels in the frame, a rectangle was drawm and a point was identified in the middle of this rectangle. The isolated rectangle can be seen in Appendix \ref{appendix:isolated_rectangle}.

After the centre point of the rectangle has been identified, the angle of the pendulum to its centre point was calculated. This was done using basic trigonometric identities using the x and y coordinates of the point. This calculation is show for radians in Fig \ref{fig:CW1_Task_2_code_locating_angle_calculation_radians}. 

\begin{figure}
\centerline{\includegraphics[width=.5\textwidth]{CW1_Task_2_code_locating_angle_calculation_radians}}
\caption{Calculating the angle of the pendulum in radians}
\label{fig:CW1_Task_2_code_locating_angle_calculation_radians}
\end{figure}

Once the angle in radians had been calculated, converting the angle to degrees was trivial. This conversion can be seen in Fig \ref{fig:CW1_Task_2_code_locating_angle_calculation_degrees}.

\begin{figure}
\centerline{\includegraphics[width=.5\textwidth]{CW1_Task_2_code_locating_angle_calculation_degrees}}
\caption{Converting Angles From Radians to Degress}
\label{fig:CW1_Task_2_code_locating_angle_calculation_degrees}
\end{figure}

Once the the angle was outputted onto the frame in both degrees and radians. This was done using the \verb|putText()| method. The text outputted onto each frame can be seen in Fig \ref{fig:example_of_output_of_angles}.

\begin{figure}
\centerline{\includegraphics[width=.5\textwidth]{example_of_output_of_angles}}
\caption{Angle Output on Video Frames}
\label{fig:example_of_output_of_angles}
\end{figure}

Alongside the output on each of the frames, the angle of the pendulum was printed to the terminal. This is done in the following way: \verb|cout << "Pendulum angle:"|
\verb| << angleInDegrees << endl;|

At the end of each loop for each frame, the angle (in degrees) is then saved to an Comma Seperated Values (CSV) file. THis was done using the following method: \verb|DataFile << angleInDegrees << endl;|

The values saved into the CSV file, were then combined into a graph to show the angle of the pendulum against time. This can be seen in Fig. \ref{fig:angle_output}.

\begin{figure}
\centerline{\includegraphics[width=.5\textwidth]{angle_output}}
\caption{Angle of Pendulum Against Time}
\label{fig:angle_output}
\end{figure}

The full size graph of angle against time can be seen in Appendix \ref{appendix:angle_output_full_size}.

It can be seen from the graph that the angle of the pendulums swing is not centered around $0$ degrees, there is an offset of about $12.5$ degrees. This is caused by the testing set up being slightly offset to the camera. This causes distortion in the results that can be seen as the offset. 

The full code for task 2 is shown in Appendix \ref{appendix:task_2_full_code}.

Whilst the code is simplistic it does manage to track the pendulum well and doesn't struggle to identify the end of the pendulum in the frame. Despite this, there was nothing else green within the frame, this may have caused issues if this were to be the case. 

\subsection{Further Improvements}

As suggested in the previous section, the improvements for this task revolve mainly around reducing the chance of another object of similar colour in the frame being detected. This could be done in many ways. The most simple way of doing this would be to reduce the chance of targeted body to be a unique shape to everything else in the image. Although \textit{OpenCV} does provide framework for recognising objects in multiple rotations \cite{ref:object_orientation}, it was be computationally less intense if the shape have multiple lines of symmetry. Another possibility for increasing the speed of image recognition is to train a neural network to detect rotated objects. \cite{9578190} 

Another way to improve results, and add complexity would be to add another distinctive colour to track and making the program look for these two adjacent colours. This approach could be scaled to a complex but robust pattern of colours. 

A final improvement could be to remove the bias using some software defined bias that could compensate for the skew of the equipment and any form of effect caused by the camera lens, such as fish-eye. \textit{OpenCV} already has baked in methods of correcting for skews such as fish-eye. \cite{ref:fish-eye}

\subsection{Conclusion}

In similar fashion to part one, this solution works well within the controlled environment of the video. For this to be a more universal approach it's robustness would need to be improved. This could be achieved by adding additional tracking methods or increased target complexity, as discussed in the section above.  

\section{Task 3: Cross Correlation} 

\subsection{Introduction}

The third task is to use cross correlation to identify missing components on a PCB. Cross correlation is a method of mathematically measuring the similarity of two signals. It is used to measure how similar two samples are, on a sample by sample basis. \cite{ref:what_is_cross_corrilation} When using \textit{OpenCV}, the process of cross correlation is known as Template Matching. \cite{ref:open_cv_templae_matching}

\subsection{Solution}
\forceindent The figure below shows my solution to the third task, the Cross Correlation. 

\subsection{Further Improvements}

\subsection{Conclusion}

\bibliographystyle{IEEEtran}
\bibliography{refs}

The code can be found on GitHub \href{https://github.com/LukeDWaller99/Aint308}{Here!} 

\onecolumn

\begin{appendix}

\subsection{Pendulum in the RGB and HSV Colour Space}

\begin{figure}[H]
\centering
\subfigure[Pendulum in the RGB Colour Space]{
\includegraphics[width=.5\textwidth]{pendulum_rgb_space}
}
\subfigure[Pendulum in the HSV Colour Space]{
\includegraphics[width=.5\textwidth]{pendulum_hsv_space}
}
\end{figure}

\label{appendix:pendulum_rgb_hsv_space}

\subsection{Isolated Rectangle at the End of the Pendulum}

\begin{figure}[H]
\centerline{\includegraphics[width=.5\textwidth]{pendulum_isolate_colours}}
\end{figure}

\label{appendix:isolated_rectangle}

\subsection{Full Size Graph of Pendulum Angle Against Time}

\begin{figure}[H]
\centerline{\includegraphics[width=1.35\textwidth, angle = 90]{angle_output}}
\end{figure}

\label{appendix:angle_output_full_size}

\subsection{Full Code - Task 2 }

\begin{figure}[H]
\centerline{\includegraphics[width=.7\textwidth]{CW1_Task_2_full_code}}
\end{figure}

\label{appendix:task_2_full_code}


\end{appendix}

\end{document}
