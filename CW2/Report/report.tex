\documentclass[conference]{IEEEtran}

\renewcommand{\familydefault}{\sfdefault}

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subfigure}
\usepackage{float}
\usepackage{indentfirst}
\usepackage[compact]{titlesec}
\usepackage{mathtools}

\setlength{\parindent}{11pt}
\newcommand{\forceindent}{\leavevmode{\parindent=1em\indent}}

\hyphenation{op-tical net-works semi-conduc-tor}

\graphicspath{{images/}}

\usepackage{hyperref}

\hypersetup
{
    colorlinks=true,
    linkcolor=black,   
    urlcolor=blue,
    citecolor=black,
}

\makeatletter
\def\endthebibliography{%
  \def\@noitemerr{\@latex@warning{Empty `thebibliography' environment}}%
  \endlist
}
\makeatother

\begin{document}



\title{AINT308 - OpenCV Assignment 2 2022}

\author{\IEEEauthorblockN{Student No. 10618407}
\IEEEauthorblockA{School of Engineering,\\Computing and Mathematics
\\University of Plymouth\\
Plymouth, Devon\\}}

\maketitle

\begin{abstract}

Machine vision is a mature technology that is becoming more prevalent within modern engineering practises. It is being utilised more in the rapidly evolving fields of autonomy and automation. This report outlines some of the functionalities of a popular C/C++ based computer visions library \href{https://opencv.org}{\textit{OpenCV}}. The Assignment has been split into two tasks; Task 4 and Task 5. The first task, Task 4, is Disparity Mapping using a pair of stereo cameras to be able to judge distances to objects. The second task, Task 5, was Self-driving Car Lane Detection from the dashcam footage taken from a car.

\end{abstract}

\subsection*{Keywords:} 
Computer Vision, OpenCV, Object Detection, C++, Lane Detection

\section{Task 4: Disparity Mapping}	
\subsection{Introduction}

The first task was to perform disparity mapping to evaluate the distance of a given object in a frame to allow a robot to navigate it's environment. Disparity Mapping allows the robot to build up a 3D view of it's environment  

\subsection{Solution}

The solution to this task was split up into multiple parts. Firstly, using the calibration images to calibrate the cameras. Secondly, using the calibration result to correct the stereo distance targets. Next, Combining the left and right images into a disparity map, with brightness indicating the distance from the camera. Penultimately, labelling the distance targets with their known distance to the camera. Use this to plot disparity against distance and write a formula relating distance to disparity. Finally, using the formula to calculate the distance for each of the unknown targets.

\subsubsection{Camera Calibration and Stereo Image Correction}

Stereo camera needs to be calibrated to one another before they can be used for stereo vision. Corrections need to be made for both physical misalignments (extrinsic error), and lens distortion(intrinsic error) \cite{Stero_Calibration}. This was done using the \href{https://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html?highlight=stereocalibration}{\texttt{SteroCalibration}} program provided by \textit{OpenCV} \cite{Book_Calibration}.

The program firstly reads in image pairs (taken from the left and right camera) of a checkerboard pattern target placed in different locations and orientations. This allows the program to create a list of intrinsics and extrinsic parameters to allow for these effects in the program. The checkerboard is of known size so the program can calibrate what it sees against what it is expecting. This known as the Bouquet stereo image calibration \cite{Bouget}.

\subsubsection{Disparity Mapping} \label{Disparity_Mapping}

The first stage of disparity mapping was to remap the images to correct for the positional/lens distortion, as mentioned above \cite{OpenCV_Remapping}. The two images were then combined into a 16 bit stereo block matcher \cite{Stereo_Block_Matching}. The code for doing this can be found in Fig \ref{fig:ImageRemapping}.

\begin{figure}[H]
\centerline{\includegraphics[width=.4\textwidth]{ImageRemapping}}
\caption{Remapping Left and Right Images into one image}
\label{fig:ImageRemapping}
\end{figure}

Once this was done, a 16 bit disparity map of the two images has been created. To measure the disparity, a rectangular region of interest was created, as seen in fig \ref{fig:RectangleOnDisparityMap}. This was created to make sure that only the disparity of the measured object was used. Whilst in the actual test the disparity map used was the 16-bit version, it does not get outputted correctly, so the 8-bit version is used for demonstration purposes.

\begin{figure}[H]
\centerline{\includegraphics[width=.4\textwidth]{RectangleOnDisparityMap}}
\caption{Rectangle Drawn On Disparity Map to Indicate Search Area}
\label{fig:RectangleOnDisparityMap}
\end{figure}

Fig \ref{Disparity_Mapping} is the main code to check the disparity for this task. The code is searching through all of the pixels in a given area (as constructed in fig \ref{fig:RectangleOnDisparityMap}). If the disparity value is larger than a certain value, it is added to the list. 

\begin{figure}[H]
\centerline{\includegraphics[width=.4\textwidth]{DisparityMappingCode}}
\caption{Disparity Mapping Code for evaluating the disparity in a image}
\label{fig:DisparityMappingCode}
\end{figure}

At the end of the run, the average value of the disparity is found, this is shown in fig \ref{fig:DistanceOutputKnownDistances}. These values were saved to a CSV files for evaluation after execution. 

\subsubsection{Labelling Known Distances and Creating Disparity Function}

From the values of measured distance and disparity shown in table \ref{table:distance_and_disparity_tables}. The graph in fig \ref{fig:DisparityGraph} was created to map the disparity over distance.

\begin{figure}[H]
\centerline{\includegraphics[width=.4\textwidth]{DisparityGraph}}
\caption{Graph of Disparity over Distance }
\label{fig:DisparityGraph}
\end{figure} 

Fig \ref{fig:DisparityGraph} has been made using the data from table \ref{table:distance_and_disparity_tables}. The value for $Bf$ was calculated by rearranging the disparity equation \ref{eq:Disparity_Equation} to equation \ref{eq:Disparity_Equation_rearranged_1}. To do this, the average of the $Distance \cdot Disparity$ was taken, as shown in equation \ref{eq:AverageDisparity}. 

\begin{equation} \label{eq:Disparity_Equation}
Disparity = \frac{B \cdot f}{Distance}
\end{equation}

\begin{equation} \label{eq:Disparity_Equation_rearranged_1}
B \cdot f = Disparity \cdot Distance
\end{equation}

\begin{equation} \label{eq:AverageDisparity}
(B \cdot f)_\textrm{average} = \frac{Distance \cdot Disparity}{Total Number of Values}
\end{equation}

\begin{table}
\begin{center}
\caption{Distance and Disparity Table}
\begin{tabular}{ || c || c || c || }
\hline
 Measured Distance & Disparity Value & Distance x Disparity\\ 
\hline
 30 & 2089.72 & 62691.6 \\  
\hline
 40 & 1566 & 62640 \\  
\hline
 50 & 1240.27 & 62013.5 \\  
\hline
 60 & 1026.87 & 61612.2 \\  
\hline
 70 & 896.546 & 62758.22 \\  
\hline
 80 & 782.35 & 62588 \\  
\hline
 90 & 703.714 & 63334.26 \\  
\hline
 100 & 626.804 & 62680.4 \\  
\hline
 110 & 558.514 & 61436.54 \\  
\hline
 120 & 483.93 & 58071.6 \\  
\hline
 130 & 449.154 & 58390.02 \\  
\hline
 140 & 421.226 & 58971.64 \\  
\hline
 150 & 396.406 & 59460.9 \\  
\hline

\end{tabular}
\label{table:distance_and_disparity_tables}
\end{center}
\end{table}

Once the value of $Bf$ had been found it was used to calculate the the distances for the known targets, as of test of the previous measurements accuracy this can be seen in equation \ref{eq:Disparity_Equation_rearranged_1}. The code for calculating the distance and outputting the result can be seen in fig \ref{fig:DistanceOutputKnownDistances}.

\begin{figure}[H]
\centerline{\includegraphics[width=.4\textwidth]{DistanceOutputKnownDistances}}
\caption{Code to output the distance measured to the object}
\label{fig:DistanceOutputKnownDistances}
\end{figure}

From the values of measured distance and disparity shown in table \ref{table:distance_values_for_known_values}, and the accuracy of the prediction was also recorded. As the distance value for the testing images was already known and the disparity value had just been calculated, it was easy to test the output of the equation for reliability and accuracy.

\begin{table}
\begin{center}
\caption{Distance Calculated from Disparity Mapping with Known Distances}
\begin{tabular}{ || c || c || c || }
\hline
 Measured Distance & Calculated Distance & \% Difference\\ 
\hline
 30 & 29.32 & 2.25 \\  
\hline
 40 & 39.13 & 2.17 \\  
\hline
 50 & 49.41 & 1.18 \\  
\hline
 60 & 59.68 & 0.54 \\  
\hline
 70 & 68.35 & 2.36 \\  
\hline
 80 & 78.35 & 2.10 \\  
\hline
 90 & 87.08 & 3.24 \\  
\hline
 100 & 97.77 & 2.23 \\  
\hline
 110 & 109.72 & 0.25 \\  
\hline
 120 & 126.63 & -5.53 \\  
\hline
 130 & 136.43 & -4.95 \\  
\hline
 140 & 145.48 & -3.91 \\  
\hline
 150 & 154.59 & -3.06 \\  
\hline

\end{tabular}
\label{table:distance_values_for_known_values}
\end{center}
\end{table}

\subsubsection{Using Disparity Formula to calculate distances for unknown targets} 
To find the distance for the unknown targets, the disparity value was firstly calculated using the same method as described above. The disparity value, along with the value previously calculated using equation \ref{eq:Disparity_Equation_rearranged_1}, can be used to calculated the distance of the object in the images. 

The method for evaluating the distances of the objects in the images was very similar to the function used in,  within the Disparity Mapping section. The main differences between the two codes are, the source of the files and the terminal output of the program. The files used in calculating the distance were images with an unknown distance to the object in the image. The output in the terminal was almost identical but the known distance of the object was not stated - as it was unknown. 

It is unclear how close the  the estimates of distance were to the actual values but given the accuracy on the known images (within 5.53\%), it would be safe to assume that these values could be considered accurate within a 6\% margin.

\subsection{Further Improvements}

Although this method was effective there are still ways in which it could be improved. The two biggest flaws with this method was how the calibration was conducted.

The distance measurement was conducted using a a box held aloft over a ruler. Whilst this did provide adequate calibration data for the disparity testing, it could have been conducted in a manor that would have made the measurements increasingly more precise. 

A clearer background would have been beneficial for disparity matching. The utilised background had lots of moving objects that were not consistent between images. This required a region of interest to be used for the disparity mapping. This resulted in a smaller area for the disparity to be measured across. Ideally a plain background would have been used to allow a larger amount of the frame to be mapped. This would increase the area for the disparity mapping thus reducing the effect of outliers within the region of interest. 

These were the two biggest areas of uncertainty in this method and thus would gain the best yield if improved.

\subsection{Conclusion}
Overall this task was hard to judge on the unknown data, as the results were not known and thus hard to tell the accuracy of the finalised results. Despite this, the known measurement disparity mapping was extremely successful, with a maximum error of 5.53\% and an average error of only 2.9\%. Although the methodology could have been improved by making the calibration data collection more precise and using a clearer background, the distances of measurement used (30cm-150cm) would negate the effects of measurement over that distance. 

\section{Task 5: Self-driving Car Lane Detection}
\subsection{Introduction}

The second task was to track the lanes in dashcam footage from the front of a car driving on the road. This is one of the most complex computer vision applications within the field of autonomous cars. Mordern cars have a plethora of cars around their circumference, this allows them to capture large amounts of data whilst on the road. The real difficulty with this is how to break down the information into smaller useful chunks.  

\subsection{Solution}

\subsubsection{Use a method to detect the lane markings}

The first part of lane detection was to convert the video frames to grey scale, this was done to reduce the informational load to compute. Although Canny Edge detection works less effectively using a grey scale image \cite{Canny_On_Grey}, the lines for the edges of the road were still found and detected as edges. The frames are converted to grey scale using OpenCV's \href{https://docs.opencv.org/4.x/d8/d01/group__imgproc__color__conversions.html#ga397ae87e1288a81d2363b61574eb8cab}{\texttt{cvtColor}} function as seen in fig \ref{fig:cvtColor}.

\begin{figure}[H]
\centerline{\includegraphics[width=.4\textwidth]{cvtColor}}
\caption{Code to Convert from colour to grey scale}
\label{fig:cvtColor}
\end{figure}

After the frames are converted to grey scale, the frame is then blurred using OpenCV's \href{https://docs.opencv.org/4.x/d4/d86/group__imgproc__filter.html#ga8c45db9afe636703801b0b2e440fce37}{\texttt{blur}} function -- insert REF -- insert fig of the code. This was done to reduce the noise in the image, this makes the edge detection perform better \cite{Image_Blurring}.

\begin{figure}[H]
\centerline{\includegraphics[width=.4\textwidth]{blur}}
\caption{Code to Blur Frame}
\label{fig:blur}
\end{figure} 

Once the  image was blurred, a rectangular mask was placed over the top half of the image using the \href{https://docs.opencv.org/4.x/d6/d6e/group__imgproc__draw.html#ga07d2f74cadcf8e305e810ce8eed13bc9}{\texttt{rectangle}} function - insert REF -- insert fig of the code. This was used to eliminate the top half of the image which would not be useful for finding the road markings. 

\begin{figure}[H]
\centerline{\includegraphics[width=.4\textwidth]{rectangle}}
\caption{Code to Create a rectangle over a frame}
\label{fig:rectangle}
\end{figure}
 
Canny Edge detection was then conducted on the masked frame using OpenCV's \href{https://docs.opencv.org/4.x/dd/d1a/group__imgproc__feature.html#ga04723e007ed888ddf11d9ba04e2232de}{\texttt{Canny}} method - insert OPENCV REF here -- insert fig of the code. Canny Edge was chosen because it is deemed to be one of the best edge detection algorithms \cite{Canny2009CannyED}. Canny Edge detection uses non-maximum suppression \cite{non-maximum_supression} and Hysteresis Processes \cite{Hysteresis_Thresholding}. Non-maximum Suppression and Hysteresis Process reduce the number of false edges and thus create a better starting point for further processing such as Hough Transforms. 

\begin{figure}[H]
\centerline{\includegraphics[width=.4\textwidth]{Canny}}
\caption{Code to Run Canny edge detection}
\label{fig:Canny}
\end{figure}

After the Canny Edge detection, Hough Transforms are used as a method of extracting features from an image. It can be used to isolate features of regular curves such as, circles, lines, and ellipses. At it's simplest implementation, Hough Transforms can be used to detect straight lines, like the lines at the side of a road. Computational complexity deters the use of Hough transforms in a lot of situations. However,for finding straight lines, the most simplistic use case, Hough Lines were the chosen method of feature extraction \cite{Hough_Transform}. Simple Hough Transforms are more resilient to noise in the frame \cite{Hough_Lines_Better_Against_Noise}

The specific implementation used chosen was \href{https://docs.opencv.org/3.4/d9/db0/tutorial_hough_lines.html}{\texttt{HoughLines}}, which can be seen in fig \ref{fig:HoughLines}.

\begin{figure}[H]
\centerline{\includegraphics[width=.4\textwidth]{HoughLines}}
\caption{Code to Run Hough line detection}
\label{fig:HoughLines}
\end{figure}

Fig \ref{fig:Polar_Coordinates_for_Hough_transforms} show the representation of Hough Lines in polar coordinate form. This is used to show the line as a singular point$(\rho, \theta)$ in the hough space. 

The Hough Transform constrcuts an MxN array (where M is the distance from the origin $\rho$, and N is the corresponding angle from the origin $\theta$). These values for $\rho$ and $\theta$ can be used to find the corrinsponding x and y coordinates for each point along the lines. This is how the lines were extracted to draw on the edges of the road. 

\begin{figure}[H]
\centering
\subfigure[Image Space]{
\includegraphics[width=.225\textwidth]{Hough_Transform_Diagram_Image_Space}
}
\subfigure[Hough Space]{
\includegraphics[width=.125\textwidth]{Hough_Transform_Diagram_Hough_Space}
}
\caption{Polar Coordinates for Hough Spaceh}
\label{fig:Polar_Coordinates_for_Hough_transforms}
\end{figure} 

\subsubsection{Display a video with lanes detected}

Once the polar coordinates were instantiated, they were evaluated and the lanes were extracted from this. 

Initially a group of variables were created to calculate the lanes from the lines of the detection algorithm. 

\begin{figure}[H]
\centerline{\includegraphics[width=.4\textwidth]{setup_variables_for_lane_detection}}
\caption{Set up Variables for Lane Detection and Plotting}
\label{fig:setup_variables_for_lane_detection}
\end{figure}

After the initialisation of all the varibales shown in fig \ref{fig:setup_variables_for_lane_detection}, the algorithm runs through various loops. 

The first loop, \texttt{(i<lines.size())}, is used to iterate through all of the lines found by the Hough Lines algorithm to do lane detection on each of the lines. The second loop, \texttt{(g>Frame.rows-300)}, loops through all of the y coordinates of the lines in the images. This loop starts at the top of the page and iterates upwards to the cut off point of the denoted by \texttt{Frame.rows-300}. This was done to exclude any lines, or parts of lines, that were above the road and thus not relevant for the lane detection. The third and final loop, \texttt{(k<q)}, is used to iterate through the lines to be used in calculating the x coordinates of the points. 

\begin{figure}[H]
\centerline{\includegraphics[width=.4\textwidth]{itteration_loops_for_lane_detection}}
\caption{Loops for lane detection algorithm}
\label{fig:itteration_loops_for_lane_detection}
\end{figure}

\begin{gather} \label{eq:x_from_y}
x = \left(\frac{\rho}{sin(\theta)}\right) - (y \cdot tan(\theta))
\shortintertext{where}
\begin{aligned}
&x \text{ is x coordinate of the point on the line}\\
&y \text{ is y coordinate of the point on the line}\\
&\rho \text{ is the perpendicular distance from the origin}\\
&\theta \text{ is the angle about the origin (in radians)}\\
\end{aligned}\notag
\end{gather}


Fig \ref{fig:checking_angles_of_lines_1} shows the process of finding the left hand side lane of the road. It starts by evaluating whether the angle of the line (in radians) to the origin is less than 1. If the angle is less than 1, equation \ref{eq:x_from_y} to calculate the x value of the given point on the line. This equation uses 

\begin{figure}[H]
\centerline{\includegraphics[width=.4\textwidth]{checking_angles_of_lines_1}}
\caption{checking angles of lines 1}
\label{fig:checking_angles_of_lines_1}
\end{figure}

\begin{figure}[H]
\centerline{\includegraphics[width=.4\textwidth]{checking_angles_of_lines_2_3}}
\caption{checking angles of lines 2.3}
\label{fig:checking_angles_of_lines_2.3}
\end{figure}

\begin{figure}[H]
\centerline{\includegraphics[width=.4\textwidth]{line_in_middle_of_road}}
\caption{Line in middle of road}
\label{fig:line_in_middle_of_road}
\end{figure}

\begin{figure}[H]
\centerline{\includegraphics[width=.4\textwidth]{polygon_of_lanes}}
\caption{Polygon of Lanes}
\label{fig:polygon_of_lanes}
\end{figure}

\subsection{Further Improvements}

\subsection{Conclusion}

\bibliographystyle{IEEEtran}
\bibliography{refs}

The code can be found on GitHub \href{https://github.com/LukeDWaller99/Aint308}{here!} 

\onecolumn

\begin{appendix}

\end{appendix}

\end{document}
